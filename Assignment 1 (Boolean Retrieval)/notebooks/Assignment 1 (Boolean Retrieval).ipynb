{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import nltk\n",
    "import operator\n",
    "import pickle as pkl\n",
    "import progressbar\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = re.sub(\"[^a-zA-Z]+\", \" \", text)\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    return tokens        \n",
    "\n",
    "def preprocessing_txt(text):\n",
    "    \n",
    "    tokens = tokenizer(text)\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    new_text = \"\"\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        if token not in stopwords:\n",
    "#             print token\n",
    "            new_text += stemmer.stem(token)\n",
    "            new_text += \" \"\n",
    "        \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inverted_index():\n",
    "    \"\"\"\n",
    "    Creates a dictionary of words as key and name of the documents as items\n",
    "    \"\"\"\n",
    "    inverted = {}\n",
    "    docs_indexed = 0\n",
    "    list_doc = os.listdir(\"./alldocs\")\n",
    "    total = len(list_doc)\n",
    "    point = total / 100\n",
    "    increment = total / 100\n",
    "    indexer = {}\n",
    "    for doc in list_doc:\n",
    "#         sys.stdout.write('\\r')\n",
    "        doc_loc = \"./alldocs/\" + str(doc)\n",
    "        file_doc = open(doc_loc, \"r\")\n",
    "        file_doc = preprocessing_txt(file_doc.read())\n",
    "        tokens = tokenizer(file_doc)\n",
    "        for word in tokens:\n",
    "            if not inverted.__contains__(word):\n",
    "                count = 1\n",
    "                doclist = {}\n",
    "                doclist[doc] = 1\n",
    "                inverted[word] = doclist\n",
    "            else:\n",
    "                if doc in inverted[word]:\n",
    "                    doclist = inverted[word]\n",
    "                    doclist[doc] += 1\n",
    "                    inverted[word] = doclist\n",
    "                else:\n",
    "                    count = 1\n",
    "                    doclist = inverted[word]\n",
    "                    doclist[doc] = count\n",
    "                    inverted[word] = doclist\n",
    "                    \n",
    "        docs_indexed += 1\n",
    "        i = docs_indexed\n",
    "        if(i % (point) == 0):\n",
    "            sys.stdout.write(\"\\r[\" + \"=\" * (i / increment) + \">\" +  \" \" * ((total - i)/ increment) + \"]\" +  str(100*i / float(len(list_doc))) + \"%\")\n",
    "            sys.stdout.flush()\n",
    "    return inverted\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====================================================================================================>]99.749176987%"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    with open(\"indexed_docs.p\",\"wb\") as handle:\n",
    "        indexed_docs = inverted_index()\n",
    "        pkl.dump(indexed_docs, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"indexed_docs.p\",\"r\") as dict_file:\n",
    "    dictionary = pkl.load(dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_query(query_file):\n",
    "    \n",
    "    f = open(query_file,\"r\")\n",
    "    query_ID = []\n",
    "    query_result = []\n",
    "    Time = []\n",
    "    q_res = pd.DataFrame(columns = [\"queryID\",\"Time\"])\n",
    "    for j,query in enumerate(f):\n",
    "        result = []\n",
    "        q_ID = query.split()[0]\n",
    "        query_ID.append(q_ID)\n",
    "        query = preprocessing_txt(query)\n",
    "        query = query.split()\n",
    "        START = time.time()\n",
    "        for i,word in enumerate(query):\n",
    "            \n",
    "            if i == 0:\n",
    "                for i in dictionary[word].items():\n",
    "                    result.append(i[0])\n",
    "            else:\n",
    "                temp = []\n",
    "                for i in dictionary[word].items():\n",
    "                    temp.append(i[0])\n",
    "                result = list(set(result).intersection(set(temp)))\n",
    "        END = time.time()\n",
    "        f1 = open(\"output_inverted_index.txt\",'a')\n",
    "#         print str(q_ID) + \"%d\" % len(result)\n",
    "        for res in result:\n",
    "            f1.write(str(q_ID) + \" \" + str(res) + \"\\n\")\n",
    "        f1.close() \n",
    "        Time.append(float(END - START))\n",
    "        query_result.append(result)\n",
    "    q_res[\"queryID\"] = query_ID\n",
    "    q_res[\"Time\"] = Time\n",
    "    q_res.to_csv(\"inverted_index.csv\",encoding='utf-8')\n",
    "    result = pd.DataFrame(columns = [\"query_ID\",\"relevant_docs\"])\n",
    "    result[\"query_ID\"] = query_ID\n",
    "    result[\"relevant_docs\"] = query_result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    query_file = \"query.txt\"\n",
    "    process_query(query_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def precision_and_recall(output_file,filename):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        output_file: file containing result for queries\n",
    "    \"\"\"\n",
    "    \n",
    "    output = open(\"output.txt\",'r')\n",
    "    query = open(\"query.txt\",\"r\")\n",
    "    est_output = open(output_file)\n",
    "    query_ID = []\n",
    "    prerec = open(filename,\"a\")\n",
    "    prerec.write(\"queryID\" + \" \" + \"precision\" + \" \" + \"recall\" + \"\\n\")\n",
    "    for line in query:\n",
    "        query_ID.append(line.split()[0])\n",
    "    e_out = pd.DataFrame(columns = [\"q_ID\",\"Doc\"])\n",
    "    o_out = pd.DataFrame(columns=[\"q_ID\",\"Doc\"])\n",
    "    query = []\n",
    "    docs = []\n",
    "    for line in est_output:\n",
    "        query.append(line.split()[0])\n",
    "        docs.append(line.split()[1])\n",
    "    e_out['q_ID'] = query\n",
    "    e_out[\"Doc\"] = docs\n",
    "    query = []\n",
    "    docs = []\n",
    "    for line in output:\n",
    "        query.append(line.split()[0])\n",
    "        docs.append(line.split()[1])\n",
    "    o_out['q_ID'] = query\n",
    "    o_out[\"Doc\"] = docs\n",
    "        \n",
    "    for q_ID in query_ID:\n",
    "        estimated = list(e_out[e_out['q_ID'] == q_ID][\"Doc\"])\n",
    "#         print len(estimated)\n",
    "        true = list(o_out[o_out[\"q_ID\"] == q_ID][\"Doc\"])\n",
    "#         print len(true)\n",
    "\n",
    "        if len(estimated) == 0:\n",
    "            precision = 0\n",
    "        else:    \n",
    "            precision = len(list(set(estimated).intersection(set(true))))/float(len(estimated))\n",
    "        recall = len(list(set(estimated).intersection(set(true))))/float(len(true))\n",
    "        prerec.write(str(q_ID) + \" \" + str(precision) + \" \" + str(recall) + \"\\n\")\n",
    "    prerec.close()\n",
    "    output.close()\n",
    "    est_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precision_and_recall(\"output_inverted_index.txt\",\"inverted_index_precision_and_recall.txt\")\n",
    "precision_and_recall(\"output_grep.txt\",\"grep_precision_and_recall.txt\")\n",
    "precision_and_recall(\"output_elastic.txt\",\"elastic_search_precision_and_recall.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495\n",
      "1\n",
      "72\n",
      "1\n",
      "476\n",
      "46\n",
      "179\n",
      "11\n",
      "17\n",
      "28\n",
      "1\n",
      "16\n",
      "65\n",
      "2\n",
      "38\n",
      "108\n",
      "3\n",
      "19\n",
      "40\n",
      "2\n",
      "2\n",
      "65\n",
      "3\n",
      "1\n",
      "23\n",
      "7\n",
      "30\n",
      "431\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-216-abc5e6a43e32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'query.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-216-abc5e6a43e32>\u001b[0m in \u001b[0;36mprocessing\u001b[0;34m(queries)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"grep\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'-lr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'alldocs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "__file__\n",
    "\n",
    "    grep_search.py\n",
    "\n",
    "__description__\n",
    "\n",
    "    This file contains the method of using grep to search for queries using python\n",
    "\n",
    "__author__\n",
    "\n",
    "    Kaustubh Mani <kaustubh3095@gmail.com>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "\n",
    "def processing(queries):\n",
    "    for lines in queries :\n",
    "        final_set = set()\n",
    "        Start = time.time()\n",
    "        query_id = lines[0:3]\n",
    "        lines = lines[4:]\n",
    "        words = lines.split(' ')\n",
    "        final_set = set()\n",
    "        count = 0\n",
    "        \n",
    "        for word in words:\n",
    "            process=Popen([\"grep\",'-lr',word,'alldocs'],stdout=PIPE)\n",
    "            s=str(process.stdout.read())\n",
    "            temp=s.split(\"\\n\")\n",
    "            if count == 0:\n",
    "                final_set=set(temp)\n",
    "                count = 1\n",
    "            else:\n",
    "                final_set = final_set & set(temp)\n",
    "        final_answer = list(final_set)\n",
    "        print len(final_answer)\n",
    "        \n",
    "        file = open('grep_output.txt','a')\n",
    "        for answers in final_answer :\n",
    "            if answers == \"\":\n",
    "                continue\n",
    "            file.write(str(query_id)+\"  \"+str(answers.split('/')[-1])+\"\\n\")\n",
    "        file.close()\n",
    "        \n",
    "        file = open('grep_time.txt','a')\n",
    "        file.write(str(query_id)+\"  \"+str(time.time()-Start)+\"\\n\")\n",
    "        file.close()\n",
    "        # break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    queries = open('query.txt','r')\n",
    "    processing(queries)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
